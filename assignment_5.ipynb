{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb83b66f-e3cb-45ee-bdb1-601f3b5dd03b",
   "metadata": {},
   "source": [
    "Generate a dataset with atleast seven highly correlated columns and a target variable. Implement Ridge Regression using Gradient Descent Optimization. Take different values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization parameter (10-15,10-10,10-5,10- 3,0,1,10,20). Choose the best parameters for which ridge regression cost function is minimum and R2_score is maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1550c33-a840-4aca-9fd8-c4954bbe27fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lr     alpha status  final_cost  mse_test   r2_test  iters\n",
      "10  0.001   1.00000     ok    0.120167  0.219727  0.989340  20000\n",
      "9   0.001   0.10000     ok    0.117177  0.219742  0.989339  20000\n",
      "8   0.001   0.01000     ok    0.116878  0.219744  0.989339  20000\n",
      "7   0.001   0.00100     ok    0.116848  0.219744  0.989339  20000\n",
      "6   0.001   0.00001     ok    0.116844  0.219744  0.989339  20000\n",
      "16  0.010   1.00000     ok    0.120081  0.219756  0.989338  20000\n",
      "11  0.001  10.00000     ok    0.149976  0.219757  0.989338  20000\n",
      "17  0.010  10.00000     ok    0.149970  0.219761  0.989338   8772\n",
      "23  0.100  10.00000     ok    0.149970  0.219762  0.989338   1336\n",
      "15  0.010   0.10000     ok    0.117040  0.219779  0.989337  20000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- synthetic data -----\n",
    "np.random.seed(0)\n",
    "n = 500\n",
    "z = np.random.randn(n, 1)\n",
    "X = np.hstack([z + 0.01 * np.random.randn(n, 1) for _ in range(8)])\n",
    "X = np.hstack([X, np.random.randn(n, 2)])\n",
    "true_w = np.array([2.5, -1.2, 1.8, 0.0, 0.7, -0.5, 1.0, 0.3, 0.0, 0.0])\n",
    "y = X.dot(true_w) + 0.5 * np.random.randn(n)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# ----- preprocessing -----\n",
    "scaler = StandardScaler()\n",
    "X_tr_s = scaler.fit_transform(X_tr)\n",
    "X_te_s = scaler.transform(X_te)\n",
    "\n",
    "# ----- ridge regression helpers -----\n",
    "def ridge_cost_grad(w, X, y, alpha):\n",
    "    m = X.shape[0]\n",
    "    Xb = np.c_[np.ones((m, 1)), X]\n",
    "    preds = Xb.dot(w)\n",
    "    err = preds - y\n",
    "    cost = (1/(2*m)) * np.sum(err**2) + (alpha/(2*m)) * np.sum(w[1:]**2)\n",
    "    grad = (1/m) * Xb.T.dot(err)\n",
    "    grad[1:] += (alpha/m) * w[1:]\n",
    "    return cost, grad\n",
    "\n",
    "def ridge_gd_stable(X, y, alpha=1.0, lr=1e-3, n_iter=20000, clip=1e3, tol=1e-9, verbose=False):\n",
    "    m, d = X.shape\n",
    "    w = np.zeros(d + 1)\n",
    "    costs = []\n",
    "    for i in range(n_iter):\n",
    "        cost, grad = ridge_cost_grad(w, X, y, alpha)\n",
    "        if not np.isfinite(cost):\n",
    "            if verbose: print(f\"Stop: non-finite cost at iter {i}\")\n",
    "            return None, costs, False\n",
    "        grad = np.clip(grad, -clip, clip)\n",
    "        w -= lr * grad\n",
    "        costs.append(cost)\n",
    "        if not np.all(np.isfinite(w)):\n",
    "            if verbose: print(f\"Stop: non-finite weights at iter {i}\")\n",
    "            return None, costs, False\n",
    "        if i > 1 and abs(costs[-2] - costs[-1]) < tol:\n",
    "            return w, costs, True\n",
    "    return w, costs, True\n",
    "\n",
    "# ----- hyperparam sweep -----\n",
    "lrs = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "alphas = [1e-5, 1e-3, 0.01, 0.1, 1, 10]\n",
    "summary = []\n",
    "\n",
    "for lr in lrs:\n",
    "    for a in alphas:\n",
    "        w, history, ok = ridge_gd_stable(X_tr_s, y_tr, alpha=a, lr=lr, n_iter=20000, clip=1e4, tol=1e-10)\n",
    "        if not ok or w is None:\n",
    "            summary.append({'lr': lr, 'alpha': a, 'status': 'fail'})\n",
    "            continue\n",
    "        Xb_te = np.c_[np.ones((X_te_s.shape[0], 1)), X_te_s]\n",
    "        preds = Xb_te.dot(w)\n",
    "        if not np.all(np.isfinite(preds)):\n",
    "            summary.append({'lr': lr, 'alpha': a, 'status': 'bad_preds'})\n",
    "            continue\n",
    "        mse = mean_squared_error(y_te, preds)\n",
    "        r2 = r2_score(y_te, preds)\n",
    "        summary.append({\n",
    "            'lr': lr, 'alpha': a, 'status': 'ok',\n",
    "            'final_cost': history[-1], 'mse_test': mse, 'r2_test': r2,\n",
    "            'iters': len(history)\n",
    "        })\n",
    "\n",
    "results = pd.DataFrame(summary)\n",
    "print(results.sort_values(['status', 'r2_test'], ascending=[True, False]).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee5da2-5b50-4b68-8573-e6ed195cfb1e",
   "metadata": {},
   "source": [
    "#### Load the Hitters dataset from the following link\n",
    "https://gist.githubusercontent.com/keeganhines/59974f1ebef97bbaa44fb19143f90bad/raw/Hitters.csv\n",
    "\n",
    "(a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
    "\n",
    "(b) Separate input and output features and perform scaling\n",
    "\n",
    "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use regularization parameter as 0.5748) regression function on the dataset.\n",
    "\n",
    "(d) Evaluate the performance of each trained model on test set. Which model performs the best and Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c884bed-9ab5-4230-bcd4-11198f94fa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (322, 21)\n",
      "          Unnamed: 0  AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  \\\n",
      "0     -Andy Allanson    293    66      1    30   29     14      1     293   \n",
      "1        -Alan Ashby    315    81      7    24   38     39     14    3449   \n",
      "2       -Alvin Davis    479   130     18    66   72     76      3    1624   \n",
      "3      -Andre Dawson    496   141     20    65   78     37     11    5628   \n",
      "4  -Andres Galarraga    321    87     10    39   42     30      2     396   \n",
      "\n",
      "   CHits  ...  CRuns  CRBI  CWalks  League Division PutOuts  Assists  Errors  \\\n",
      "0     66  ...     30    29      14       A        E     446       33      20   \n",
      "1    835  ...    321   414     375       N        W     632       43      10   \n",
      "2    457  ...    224   266     263       A        W     880       82      14   \n",
      "3   1575  ...    828   838     354       N        E     200       11       3   \n",
      "4    101  ...     48    46      33       N        E     805       40       4   \n",
      "\n",
      "   Salary  NewLeague  \n",
      "0     NaN          A  \n",
      "1   475.0          N  \n",
      "2   480.0          A  \n",
      "3   500.0          N  \n",
      "4    91.5          N  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Categorical cols: ['League', 'Division', 'NewLeague']\n",
      "Linear: RMSE=358.168 | R²=0.291\n",
      "Ridge: RMSE=355.646 | R²=0.301\n",
      "Lasso: RMSE=355.595 | R²=0.301\n"
     ]
    }
   ],
   "source": [
    "#Q2 hitters regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# --- Load dataset ---\n",
    "url = \"https://gist.githubusercontent.com/keeganhines/59974f1ebef97bbaa44fb19143f90bad/raw/Hitters.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# --- Clean and preprocess ---\n",
    "df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')\n",
    "df = df.dropna(subset=['Salary']).reset_index(drop=True)\n",
    "\n",
    "# Drop name column if it exists\n",
    "if df.columns[0].lower() not in [c.lower() for c in ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI']]:\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "cat_cols = [c for c in df.select_dtypes(include=['object']).columns if c not in ['Player', 'Name']]\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "\n",
    "X = df.drop(columns=['Salary'])\n",
    "y = df['Salary'].values\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols)\n",
    "])\n",
    "\n",
    "# --- Split & train ---\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "alpha = 0.5748\n",
    "pipelines = {\n",
    "    'Linear': Pipeline([('prep', preproc), ('model', LinearRegression())]),\n",
    "    'Ridge' : Pipeline([('prep', preproc), ('model', Ridge(alpha=alpha))]),\n",
    "    'Lasso' : Pipeline([('prep', preproc), ('model', Lasso(alpha=alpha, max_iter=10000))])\n",
    "}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    preds = pipe.predict(X_te)\n",
    "    mse = mean_squared_error(y_te, preds)\n",
    "    r2 = r2_score(y_te, preds)\n",
    "    print(f\"{name}: RMSE={np.sqrt(mse):.3f} | R²={r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1bc8cd-a4fb-45cb-8ca8-0304a1a33876",
   "metadata": {},
   "source": [
    "#### Cross Validation for Ridge and Lasso Regression\n",
    "\n",
    "Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV) function of Python. Implement both on Boston House Prediction Dataset (load_boston dataset from sklearn.datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afedc7fc-7f8c-4c78-9569-27b653e22ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV - best α = 26.126752\n",
      "R² = 0.6905 | MSE = 16.0824\n",
      "\n",
      "LassoCV - best α = 0.047088\n",
      "R² = 0.6948 | MSE = 15.8604\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# --- Load dataset ---\n",
    "try:\n",
    "    from sklearn.datasets import load_boston\n",
    "    boston = load_boston()\n",
    "    X, y = boston.data, boston.target\n",
    "except Exception:\n",
    "    url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "    raw = pd.read_csv(url, sep=r\"\\s+\", header=None, skiprows=22)\n",
    "    data = np.hstack([raw.values[::2, :], raw.values[1::2, :2]])\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# --- Scale and split ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Ridge Regression with CV ---\n",
    "alpha_grid = np.logspace(-6, 6, 200)\n",
    "ridge = RidgeCV(alphas=alpha_grid, store_cv_results=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "ridge_preds = ridge.predict(X_test)\n",
    "print(f\"RidgeCV - best α = {ridge.alpha_:.6f}\")\n",
    "print(f\"R² = {r2_score(y_test, ridge_preds):.4f} | MSE = {mean_squared_error(y_test, ridge_preds):.4f}\")\n",
    "\n",
    "# --- Lasso Regression with CV ---\n",
    "lasso = LassoCV(cv=5, max_iter=10000)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lasso_preds = lasso.predict(X_test)\n",
    "print(f\"\\nLassoCV - best α = {lasso.alpha_:.6f}\")\n",
    "print(f\"R² = {r2_score(y_test, lasso_preds):.4f} | MSE = {mean_squared_error(y_test, lasso_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f993d81-9e63-4d75-82c5-f2108420dfad",
   "metadata": {},
   "source": [
    "#### Multiclass Logistic Regression:\n",
    "Implement Multiclass Logistic Regression (step-by step) on Iris dataset using one vs. rest strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd59ca1-fe2e-479e-8e17-e354a977ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- Load data ---\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# --- Scale & split ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Sigmoid & training ---\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def fit_binary_logistic(X, y_bin, lr=0.1, steps=5000, tol=1e-6):\n",
    "    m, n = X.shape\n",
    "    Xb = np.c_[np.ones((m, 1)), X]\n",
    "    w = np.zeros(n + 1)\n",
    "    for _ in range(steps):\n",
    "        preds = sigmoid(Xb @ w)\n",
    "        grad = (1/m) * Xb.T.dot(preds - y_bin)\n",
    "        w -= lr * grad\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "    return w\n",
    "\n",
    "# --- Train one-vs-rest classifiers ---\n",
    "K = len(np.unique(y_tr))\n",
    "W = []\n",
    "for k in range(K):\n",
    "    y_k = (y_tr == k).astype(int)\n",
    "    w_k = fit_binary_logistic(X_tr, y_k, lr=0.3, steps=10000)\n",
    "    W.append(w_k)\n",
    "W = np.vstack(W)\n",
    "\n",
    "# --- Predict ---\n",
    "def predict_multiclass(X, W):\n",
    "    Xb = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    probs = sigmoid(Xb @ W.T)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "y_pred = predict_multiclass(X_te, W)\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(f\"Accuracy: {accuracy_score(y_te, y_pred):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_te, y_pred, target_names=iris.target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_te, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
